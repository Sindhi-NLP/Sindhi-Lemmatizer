{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G4PKfmFx5n2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference and Imports\n",
        "\n",
        "This section covers the setup for running inference using a pre-trained sequence-to-sequence model. The process includes importing necessary libraries, defining functions to load the models and tokenizers, and utilizing a function to lemmatize Sindhi words.\n",
        "\n",
        "#### Steps and Explanation\n",
        "\n",
        "1. **Import Libraries**:\n",
        "   - Import essential libraries including `numpy`, `sklearn`, and components from `tensorflow.keras`.\n",
        "\n",
        "2. **Load Models and Tokenizers**:\n",
        "   - Define a function `load_models_and_tokenizers` to load the pre-trained encoder and decoder models, input and target tokenizers, and the configuration dictionary from saved files.\n",
        "\n",
        "3. **Lemmatize Function**:\n",
        "   - Define a `lemmatize` function to convert input text into sequences, pad them, and predict the lemma using the loaded models. This function implements the inference logic of the sequence-to-sequence model.\n",
        "\n",
        "4. **Test the Lemmatizer**:\n",
        "   - Load the models and tokenizers using the `load_models_and_tokenizers` function.\n",
        "   - Test the lemmatizer on a set of Sindhi words to verify its functionality."
      ],
      "metadata": {
        "id": "wbjitQwz64oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "import pickle"
      ],
      "metadata": {
        "id": "OQfPhhXt4zMk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Models and Tokenizers\n",
        "\n",
        "This function loads the trained encoder and decoder models along with their corresponding tokenizers and configuration settings. This step is crucial for performing inference using the trained POS tagging model. Here's a breakdown of the code:\n",
        "\n",
        "#### Code Steps and Explanation\n",
        "\n",
        "1. **Loading Encoder and Decoder Models**:\n",
        "   - `encoder_model = load_model('encoder_model.h5')`: Loads the saved encoder model.\n",
        "   - `decoder_model = load_model('decoder_model.h5')`: Loads the saved decoder model.\n",
        "\n",
        "2. **Loading Input Tokenizer**:\n",
        "   - `with open('input_tokenizer.pickle', 'rb') as handle: input_tokenizer = pickle.load(handle)`: Opens and loads the input tokenizer from the saved pickle file.\n",
        "\n",
        "3. **Loading Target Tokenizer**:\n",
        "   - `with open('target_tokenizer.pickle', 'rb') as handle: target_tokenizer = pickle.load(handle)`: Opens and loads the target tokenizer from the saved pickle file.\n",
        "\n",
        "4. **Loading Configuration**:\n",
        "   - `with open('config.pickle', 'rb') as handle: config = pickle.load(handle)`: Opens and loads the configuration settings from the saved pickle file.\n",
        "\n",
        "5. **Returning Loaded Objects**:\n",
        "   - `return encoder_model, decoder_model, input_tokenizer, target_tokenizer, config`: Returns the loaded encoder model, decoder model, input tokenizer, target tokenizer, and configuration settings."
      ],
      "metadata": {
        "id": "rwwxfZ885WHn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AUvjyU743q3m"
      },
      "outputs": [],
      "source": [
        "def load_models_and_tokenizers():\n",
        "    encoder_model = load_model('encoder_model.h5')\n",
        "    decoder_model = load_model('decoder_model.h5')\n",
        "\n",
        "    with open('input_tokenizer.pickle', 'rb') as handle:\n",
        "        input_tokenizer = pickle.load(handle)\n",
        "\n",
        "    with open('target_tokenizer.pickle', 'rb') as handle:\n",
        "        target_tokenizer = pickle.load(handle)\n",
        "\n",
        "    with open('config.pickle', 'rb') as handle:\n",
        "        config = pickle.load(handle)\n",
        "\n",
        "    return encoder_model, decoder_model, input_tokenizer, target_tokenizer, config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization Function\n",
        "\n",
        "This function performs lemmatization on an input text using the trained encoder-decoder model architecture. It translates the input sequence into its lemmatized form by leveraging the trained sequence-to-sequence model.\n",
        "\n",
        "#### Code Steps and Explanation\n",
        "\n",
        "1. **Tokenizing and Padding Input Text**:\n",
        "   - `input_seq = input_tokenizer.texts_to_sequences([input_text])`: Converts the input text into a sequence of integer tokens.\n",
        "   - `input_seq = pad_sequences(input_seq, maxlen=config['max_input_len'], padding='post')`: Pads the token sequence to the maximum input length defined in the configuration.\n",
        "\n",
        "2. **Encoding the Input**:\n",
        "   - `states_value = encoder_model.predict(input_seq)`: Encodes the input sequence and retrieves the internal states from the encoder model.\n",
        "\n",
        "3. **Initializing the Target Sequence**:\n",
        "   - `target_seq = np.zeros((1, 1))`: Initializes an empty target sequence with a length of 1.\n",
        "   - `target_seq[0, 0] = 2`: Sets the first character of the target sequence to the start token (typically represented by the integer 2).\n",
        "\n",
        "4. **Decoding the Output**:\n",
        "   - A loop is used to generate the output sequence token by token:\n",
        "     - `output_tokens, h, c = decoder_model.predict([target_seq] + states_value)`: Predicts the next token and updates the states.\n",
        "     - `sampled_token_index = np.argmax(output_tokens[0, -1, :])`: Finds the index of the most probable token.\n",
        "     - `sampled_char = target_tokenizer.index_word.get(sampled_token_index, '')`: Retrieves the actual token (character) from the token index.\n",
        "     - `decoded_sentence += sampled_char`: Appends the predicted token to the decoded sentence.\n",
        "     - The loop continues until a stop condition is met: either the end of the sequence is reached or the maximum target length is exceeded.\n",
        "\n",
        "5. **Updating Target Sequence and States**:\n",
        "   - `target_seq = np.zeros((1, 1))`: Prepares the target sequence for the next iteration.\n",
        "   - `target_seq[0, 0] = sampled_token_index`: Sets the target sequence to the sampled token index.\n",
        "   - `states_value = [h, c]`: Updates the states for the next prediction.\n",
        "\n",
        "6. **Returning the Decoded Sentence**:\n",
        "   - `return decoded_sentence`: Returns the final lemmatized sentence after the loop completes."
      ],
      "metadata": {
        "id": "gw-JLnOG5cih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(input_text, encoder_model, decoder_model, input_tokenizer, target_tokenizer, config):\n",
        "    input_seq = input_tokenizer.texts_to_sequences([input_text])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=config['max_input_len'], padding='post')\n",
        "\n",
        "    # Encode the input\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character\n",
        "    target_seq[0, 0] = 2  # start token\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = target_tokenizer.index_word.get(sampled_token_index, '')\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length or find stop character\n",
        "        if (sampled_char == '' or len(decoded_sentence) > config['max_target_len']):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "ds230ydX4v3y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Lemmatizer\n",
        "\n",
        "The following code demonstrates how to load the trained models and tokenizers, and use the lemmatization function to predict lemmas for a list of test words.\n",
        "\n",
        "#### Steps and Explanation\n",
        "\n",
        "1. **Load Models and Tokenizers**:\n",
        "   - `encoder_model, decoder_model, input_tokenizer, target_tokenizer, config = load_models_and_tokenizers()`: Loads the pre-trained encoder and decoder models, as well as the input and target tokenizers, and the configuration dictionary.\n",
        "\n",
        "2. **Test the Lemmatizer**:\n",
        "   - A list of test words (`test_words = ['ڪرڻ', 'اڪثر', 'کائيندو']`) is defined for testing the lemmatizer.\n",
        "   - For each word in the test list:\n",
        "     - `lemma = lemmatize(word, encoder_model, decoder_model, input_tokenizer, target_tokenizer, config)`: The `lemmatize` function is called to predict the lemma of the word.\n",
        "     - `print(f\"Word: {word}, Lemma: {lemma}\")`: The original word and its predicted lemma are printed."
      ],
      "metadata": {
        "id": "6NP5o22H5hkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model, decoder_model, input_tokenizer, target_tokenizer, config = load_models_and_tokenizers()\n",
        "# Test the lemmatizer\n",
        "test_words = ['ڪرڻ', 'اڪثر', 'کائيندو']\n",
        "for word in test_words:\n",
        "  lemma = lemmatize(word, encoder_model, decoder_model, input_tokenizer, target_tokenizer, config)\n",
        "  print(f\"Word: {word}, Lemma: {lemma}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBOqNuO_44m-",
        "outputId": "1f4a56ed-78f6-4f5f-ea4a-a766fcd1d806"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 513ms/step\n",
            "1/1 [==============================] - 0s 438ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Word: ڪرڻ, Lemma: ڪر\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Word: اڪثر, Lemma: اڪثر\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Word: کائيندو, Lemma: ناه\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zqopPwTE46qR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}