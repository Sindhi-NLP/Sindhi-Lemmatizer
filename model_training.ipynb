{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sindhi Language Lemmatizer using Sequence-to-Sequence Model\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook demonstrates the process of building and training a lemmatizer for the Sindhi language using a sequence-to-sequence (seq2seq) model with LSTM layers. Lemmatization is a crucial step in many natural language processing tasks, reducing words to their base or dictionary form.\n",
        "\n",
        "## Objective\n",
        "\n",
        "Our goal is to create a model that can accurately lemmatize Sindhi words, handling the complexities and nuances of the language's morphology.\n",
        "\n",
        "## Approach\n",
        "\n",
        "We'll use the following steps:\n",
        "\n",
        "1. Data Preparation: Load and preprocess the Sindhi POS dataset.\n",
        "2. Model Architecture: Implement a seq2seq model with an encoder-decoder structure.\n",
        "3. Training: Train the model on our prepared dataset.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We're using the SindhiPosDataset.csv, which contains Sindhi words along with their lemmas and other linguistic information.\n",
        "\n",
        "## Libraries Used\n",
        "\n",
        "- pandas: For data manipulation and analysis\n",
        "- numpy: For numerical operations\n",
        "- sklearn: For train-test split\n",
        "- tensorflow.keras: For building and training the neural network model\n",
        "- matplotlib: For visualizing the training process\n",
        "\n",
        "## Note\n",
        "\n",
        "This lemmatizer is designed specifically for the Sindhi language and may not generalize well to other languages without modifications. The performance of the model depends on the quality and quantity of the training data.\n",
        "\n",
        "Let's begin by importing the necessary libraries and loading our dataset!"
      ],
      "metadata": {
        "id": "27cLgocHjvZJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ewJUvX3uV4wn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "The `load_and_preprocess_data` function handles the crucial steps of preparing our Sindhi language data for the lemmatization model. Here's a breakdown of its operations:\n",
        "\n",
        "1. **Data Loading**:\n",
        "   - Reads the CSV file containing Sindhi words and their lemmas.\n",
        "\n",
        "2. **Train-Test Split**:\n",
        "   - Splits the data into training (80%) and testing (20%) sets using sklearn's `train_test_split`.\n",
        "\n",
        "3. **Tokenization**:\n",
        "   - Creates two separate tokenizers:\n",
        "     - `input_tokenizer` for the word forms ('FORM' column)\n",
        "     - `target_tokenizer` for the lemmas ('LEMMA' column)\n",
        "   - Both tokenizers operate at the character level, which is crucial for handling the morphological complexity of Sindhi.\n",
        "\n",
        "4. **Sequence Conversion**:\n",
        "   - Converts the words and lemmas into numerical sequences using the fitted tokenizers.\n",
        "\n",
        "5. **Sequence Length Determination**:\n",
        "   - Calculates the maximum length for input (word) and target (lemma) sequences.\n",
        "\n",
        "6. **Padding**:\n",
        "   - Pads all sequences to the maximum length to ensure uniform input to the neural network.\n",
        "   - Uses post-padding (adds zeros at the end of sequences).\n",
        "\n",
        "7. **Return Values**:\n",
        "   - Returns preprocessed training and testing data.\n",
        "   - Also returns the tokenizers and maximum lengths for later use in inference.\n",
        "\n",
        "This function encapsulates the entire data preparation pipeline, ensuring our Sindhi language data is appropriately formatted for training our seq2seq lemmatization model."
      ],
      "metadata": {
        "id": "NPem-VQ7kMOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "    input_tokenizer = Tokenizer(char_level=True)\n",
        "    input_tokenizer.fit_on_texts(train_df['FORM'])\n",
        "    target_tokenizer = Tokenizer(char_level=True)\n",
        "    target_tokenizer.fit_on_texts(train_df['LEMMA'])\n",
        "\n",
        "    input_train = input_tokenizer.texts_to_sequences(train_df['FORM'])\n",
        "    target_train = target_tokenizer.texts_to_sequences(train_df['LEMMA'])\n",
        "    input_test = input_tokenizer.texts_to_sequences(test_df['FORM'])\n",
        "    target_test = target_tokenizer.texts_to_sequences(test_df['LEMMA'])\n",
        "\n",
        "    max_input_len = max(len(seq) for seq in input_train + input_test)\n",
        "    max_target_len = max(len(seq) for seq in target_train + target_test)\n",
        "\n",
        "    input_train = pad_sequences(input_train, maxlen=max_input_len, padding='post')\n",
        "    target_train = pad_sequences(target_train, maxlen=max_target_len, padding='post')\n",
        "    input_test = pad_sequences(input_test, maxlen=max_input_len, padding='post')\n",
        "    target_test = pad_sequences(target_test, maxlen=max_target_len, padding='post')\n",
        "\n",
        "    return (input_train, target_train, input_test, target_test,\n",
        "            input_tokenizer, target_tokenizer, max_input_len, max_target_len)\n"
      ],
      "metadata": {
        "id": "sVaVLk15j85q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture: Sequence-to-Sequence (Seq2Seq) for Sindhi Lemmatization\n",
        "\n",
        "The `create_model` function constructs our seq2seq model for Sindhi lemmatization. This architecture is particularly suited for tasks where both input and output are sequences, like our word-to-lemma conversion.\n",
        "\n",
        "### Model Components:\n",
        "\n",
        "1. **Encoder**:\n",
        "   - Input: Accepts variable-length sequences of word characters.\n",
        "   - Embedding Layer: Converts character indices to dense vectors of fixed size.\n",
        "   - LSTM Layer: Processes the embedded sequence, capturing contextual information.\n",
        "   - Output: Produces a fixed-size context vector (final state).\n",
        "\n",
        "2. **Decoder**:\n",
        "   - Input: Accepts variable-length sequences of lemma characters.\n",
        "   - Embedding Layer: Similar to the encoder's embedding.\n",
        "   - LSTM Layer: Generates the lemma sequence, initialized with the encoder's final state.\n",
        "   - Dense Layer: Produces probability distribution over possible output characters.\n",
        "\n",
        "### Key Features:\n",
        "\n",
        "- **Shared Embedding Dimension**: Both encoder and decoder use the same embedding dimension for consistency.\n",
        "- **LSTM Units**: Determine the size of the LSTM layers in both encoder and decoder.\n",
        "- **State Transfer**: The encoder's final state initializes the decoder's LSTM, passing context.\n",
        "- **Softmax Activation**: Used in the final dense layer for character-level prediction.\n",
        "\n",
        "### Model Compilation:\n",
        "\n",
        "- **Optimizer**: RMSprop, effective for recurrent neural networks.\n",
        "- **Loss Function**: Sparse Categorical Crossentropy, suitable for character-level prediction.\n",
        "\n",
        "This architecture allows the model to learn the complex mapping between Sindhi words and their lemmas, handling variable-length inputs and outputs effectively."
      ],
      "metadata": {
        "id": "3zTx9_QEkM3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_vocab_size, target_vocab_size, embedding_dim, lstm_units):\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(lstm_units, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(target_vocab_size, embedding_dim)\n",
        "    decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "    decoder_embedded = decoder_embedding(decoder_inputs)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "    return model, encoder_inputs, encoder_states, decoder_inputs, decoder_embedding, decoder_lstm, decoder_dense"
      ],
      "metadata": {
        "id": "FDtdASJ2kAV1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Sindhi Lemmatization Model\n",
        "\n",
        "The `train_model` function is responsible for training our sequence-to-sequence model on the Sindhi lemmatization task. Here's a breakdown of its operation:\n",
        "\n",
        "### Preparation of Decoder Input\n",
        "\n",
        "1. **Decoder Input Creation**:\n",
        "   - Creates a copy of the target training data, shifted by one time step.\n",
        "   - This is a crucial step in sequence-to-sequence learning, implementing teacher forcing.\n",
        "\n",
        "2. **Start Token**:\n",
        "   - Inserts a start token (value 2) at the beginning of each decoder input sequence.\n",
        "   - This signals the beginning of decoding for each lemma.\n",
        "\n",
        "### Model Training\n",
        "\n",
        "1. **Input Data**:\n",
        "   - `input_train`: The encoded Sindhi words (encoder input).\n",
        "   - `decoder_input_data`: The prepared decoder input sequences.\n",
        "   - `target_train`: The true lemma sequences (decoder target).\n",
        "\n",
        "2. **Training Parameters**:\n",
        "   - `batch_size`: Number of samples per gradient update.\n",
        "   - `epochs`: Number of times the model will cycle through the entire dataset.\n",
        "\n",
        "3. **Validation Split**:\n",
        "   - Reserves 20% of the training data for validation.\n",
        "   - Helps monitor the model's performance on unseen data during training.\n",
        "\n",
        "4. **Model Fitting**:\n",
        "   - Uses Keras' `fit` method to train the model.\n",
        "   - Returns a `history` object containing training metrics.\n",
        "\n",
        "### Return Value\n",
        "\n",
        "- The function returns the `history` object, which can be used to plot training and validation loss curves.\n"
      ],
      "metadata": {
        "id": "AneBqk6ekNZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, input_train, target_train, batch_size, epochs):\n",
        "    decoder_input_data = np.zeros_like(target_train)\n",
        "    decoder_input_data[:, 1:] = target_train[:, :-1]\n",
        "    decoder_input_data[:, 0] = 2  # start token\n",
        "\n",
        "    history = model.fit([input_train, decoder_input_data], target_train,\n",
        "              batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "    return history"
      ],
      "metadata": {
        "id": "f3bgeLR1kDeI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Sindhi Lemmatization Model and Associated Data\n",
        "\n",
        "The `save_model_and_tokenizers` function is crucial for preserving the trained model and its associated components. This allows for later reuse without needing to retrain the model. Here's a breakdown of what this function does:\n",
        "\n",
        "### Model Saving\n",
        "\n",
        "1. **Full Model**:\n",
        "   - Saves the complete seq2seq model as 'full_model.h5'.\n",
        "   - This includes both the encoder and decoder parts of the model.\n",
        "\n",
        "2. **Encoder Model**:\n",
        "   - Saves the encoder part separately as 'encoder_model.h5'.\n",
        "   - Used for encoding input words during inference.\n",
        "\n",
        "3. **Decoder Model**:\n",
        "   - Saves the decoder part separately as 'decoder_model.h5'.\n",
        "   - Used for generating lemmas during inference.\n",
        "\n",
        "### Tokenizer Saving\n",
        "\n",
        "1. **Input Tokenizer**:\n",
        "   - Saves the tokenizer used for encoding Sindhi words.\n",
        "   - Stored as 'input_tokenizer.pickle'.\n",
        "\n",
        "2. **Target Tokenizer**:\n",
        "   - Saves the tokenizer used for encoding Sindhi lemmas.\n",
        "   - Stored as 'target_tokenizer.pickle'.\n",
        "\n",
        "### Configuration Saving\n",
        "\n",
        "- Saves important configuration parameters:\n",
        "  - `max_input_len`: Maximum length of input sequences.\n",
        "  - `max_target_len`: Maximum length of target sequences.\n",
        "- Stored as 'config.pickle'.\n",
        "\n",
        "### Technical Details\n",
        "\n",
        "- Uses the HDF5 format (.h5) for saving Keras models.\n",
        "- Utilizes Python's `pickle` module for serializing tokenizers and configuration.\n",
        "- Employs the highest pickle protocol for efficient storage.\n",
        "\n",
        "This comprehensive saving process ensures that all necessary components are preserved, allowing for easy model deployment and inference on new Sindhi words without requiring access to the original training data or retraining the model."
      ],
      "metadata": {
        "id": "Gcw_mmFYkN8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_and_tokenizers(model, encoder_model, decoder_model, input_tokenizer, target_tokenizer, max_input_len, max_target_len):\n",
        "    model.save('full_model.h5')\n",
        "    encoder_model.save('encoder_model.h5')\n",
        "    decoder_model.save('decoder_model.h5')\n",
        "\n",
        "    with open('input_tokenizer.pickle', 'wb') as handle:\n",
        "        pickle.dump(input_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    with open('target_tokenizer.pickle', 'wb') as handle:\n",
        "        pickle.dump(target_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    with open('config.pickle', 'wb') as handle:\n",
        "        pickle.dump({\n",
        "            'max_input_len': max_input_len,\n",
        "            'max_target_len': max_target_len\n",
        "        }, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "whhUBjgZkH1E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the Loss Curve\n",
        "\n",
        "In this cell, we define a function to visualize the training and validation loss of a model over epochs. This is a crucial step to understand the model's performance during training and to detect overfitting or underfitting.\n",
        "\n",
        "#### Code Explanation\n",
        "\n",
        "- **Function Definition**:\n",
        "  - `plot_loss_curve(history)`: This function takes the training history object as input and plots the loss curves for both training and validation data.\n",
        "\n",
        "- **Plotting Process**:\n",
        "  - **Figure Setup**:\n",
        "    - `plt.figure(figsize=(12, 6))`: Creates a new figure with a specified size of 12 inches by 6 inches.\n",
        "  - **Plotting Training Loss**:\n",
        "    - `plt.plot(history.history['loss'], label='Training Loss')`: Plots the training loss over epochs.\n",
        "  - **Plotting Validation Loss**:\n",
        "    - `plt.plot(history.history['val_loss'], label='Validation Loss')`: Plots the validation loss over epochs.\n",
        "  - **Title and Labels**:\n",
        "    - `plt.title('Model Loss Over Epochs')`: Sets the title of the plot.\n",
        "    - `plt.xlabel('Epoch')`: Sets the label for the x-axis.\n",
        "    - `plt.ylabel('Loss')`: Sets the label for the y-axis.\n",
        "  - **Legend**:\n",
        "    - `plt.legend()`: Adds a legend to the plot to distinguish between training and validation loss curves.\n",
        "  - **Saving the Plot**:\n",
        "    - `plt.savefig('loss_curve.png')`: Saves the plot as a PNG file named 'loss_curve.png'.\n",
        "  - **Close the Plot**:\n",
        "    - `plt.close()`: Closes the plot to free up memory."
      ],
      "metadata": {
        "id": "5vv9pt5AkOow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curve(history):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('loss_curve.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "ER6hbD0bkKQX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Saving the POS Tagging Model\n",
        "\n",
        "In this series of steps, we load and preprocess the Sindhi POS tagging dataset, create and train the model, and finally save the trained model along with the tokenizers. Here's a breakdown of the code:\n",
        "\n",
        "#### Code Steps and Explanation\n",
        "\n",
        "1. **File Path Initialization**:\n",
        "   - `file_path = \"/content/SindhiPosDataset.csv\"`: Sets the path to the Sindhi POS tagging dataset.\n",
        "\n",
        "2. **Data Loading and Preprocessing**:\n",
        "   - `load_and_preprocess_data(file_path)`: Loads and preprocesses the dataset, returning the necessary inputs and targets for training and testing, as well as tokenizers for input and target sequences, and the maximum sequence lengths.\n",
        "\n",
        "3. **Vocabulary Sizes and Model Parameters**:\n",
        "   - `input_vocab_size = len(input_tokenizer.word_index) + 1`: Computes the input vocabulary size.\n",
        "   - `target_vocab_size = len(target_tokenizer.word_index) + 1`: Computes the target vocabulary size.\n",
        "   - `embedding_dim = 128`: Sets the embedding dimension for the model.\n",
        "   - `lstm_units = 256`: Sets the number of LSTM units in the model.\n",
        "\n",
        "4. **Model Creation**:\n",
        "   - `create_model(input_vocab_size, target_vocab_size, embedding_dim, lstm_units)`: Creates the encoder-decoder model for POS tagging.\n",
        "\n",
        "5. **Model Training**:\n",
        "   - `train_model(model, input_train, target_train, batch_size=64, epochs=100)`: Trains the model with a batch size of 64 for 100 epochs, returning the training history.\n",
        "\n",
        "6. **Loss Curve Plotting**:\n",
        "   - `plot_loss_curve(history)`: Plots the loss curve for the training process to visualize the model's performance over epochs.\n",
        "\n",
        "7. **Creating Inference Models**:\n",
        "   - **Encoder Model**:\n",
        "     - `encoder_model = Model(encoder_inputs, encoder_states)`: Creates the encoder model for inference.\n",
        "   - **Decoder Model**:\n",
        "     - `decoder_state_input_h = Input(shape=(lstm_units,))`: Defines the input for the decoder's hidden state.\n",
        "     - `decoder_state_input_c = Input(shape=(lstm_units,))`: Defines the input for the decoder's cell state.\n",
        "     - `decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]`: Groups the decoder state inputs.\n",
        "     - `decoder_embedded = decoder_embedding(decoder_inputs)`: Embeds the decoder inputs.\n",
        "     - `decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedded, initial_state=decoder_states_inputs)`: Passes the embedded inputs through the LSTM layer with the initial state.\n",
        "     - `decoder_states = [state_h, state_c]`: Groups the decoder states.\n",
        "     - `decoder_outputs = decoder_dense(decoder_outputs)`: Applies the dense layer to the decoder outputs.\n",
        "     - `decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)`: Creates the decoder model for inference.\n",
        "\n",
        "8. **Saving the Models and Tokenizers**:\n",
        "   - `save_model_and_tokenizers(model, encoder_model, decoder_model, input_tokenizer, target_tokenizer, max_input_len, max_target_len)`: Saves the trained model, encoder and decoder inference models, and tokenizers to disk.\n"
      ],
      "metadata": {
        "id": "B6dgdb6ikPOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/SindhiPosDataset.csv\"\n",
        "(input_train, target_train, input_test, target_test,\n",
        "input_tokenizer, target_tokenizer, max_input_len, max_target_len) = load_and_preprocess_data(file_path)\n",
        "\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "embedding_dim = 128\n",
        "lstm_units = 256\n",
        "\n",
        "model, encoder_inputs, encoder_states, decoder_inputs, decoder_embedding, decoder_lstm, decoder_dense = create_model(\n",
        "        input_vocab_size, target_vocab_size, embedding_dim, lstm_units)\n",
        "\n",
        "history = train_model(model, input_train, target_train, batch_size=64, epochs=100)\n",
        "plot_loss_curve(history)\n",
        "    # Create inference models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(lstm_units,))\n",
        "decoder_state_input_c = Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_embedded = decoder_embedding(decoder_inputs)\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedded, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "save_model_and_tokenizers(model, encoder_model, decoder_model, input_tokenizer, target_tokenizer, max_input_len, max_target_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PblLbHreX-Gt",
        "outputId": "045bf1d8-fa67-4648-e587-12cd2f0a3d26"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "69/69 [==============================] - 16s 181ms/step - loss: 1.3559 - val_loss: 1.0655\n",
            "Epoch 2/100\n",
            "69/69 [==============================] - 12s 170ms/step - loss: 1.0337 - val_loss: 1.0037\n",
            "Epoch 3/100\n",
            "69/69 [==============================] - 12s 169ms/step - loss: 0.9409 - val_loss: 0.9023\n",
            "Epoch 4/100\n",
            "69/69 [==============================] - 11s 164ms/step - loss: 0.8875 - val_loss: 0.8812\n",
            "Epoch 5/100\n",
            "69/69 [==============================] - 10s 144ms/step - loss: 0.8299 - val_loss: 0.8344\n",
            "Epoch 6/100\n",
            "69/69 [==============================] - 12s 169ms/step - loss: 0.7790 - val_loss: 0.7352\n",
            "Epoch 7/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.7314 - val_loss: 0.6963\n",
            "Epoch 8/100\n",
            "69/69 [==============================] - 12s 169ms/step - loss: 0.6844 - val_loss: 0.6451\n",
            "Epoch 9/100\n",
            "69/69 [==============================] - 10s 149ms/step - loss: 0.6424 - val_loss: 0.6113\n",
            "Epoch 10/100\n",
            "69/69 [==============================] - 11s 158ms/step - loss: 0.6043 - val_loss: 0.5883\n",
            "Epoch 11/100\n",
            "69/69 [==============================] - 11s 167ms/step - loss: 0.5700 - val_loss: 0.5503\n",
            "Epoch 12/100\n",
            "69/69 [==============================] - 11s 166ms/step - loss: 0.5422 - val_loss: 0.5113\n",
            "Epoch 13/100\n",
            "69/69 [==============================] - 11s 159ms/step - loss: 0.5115 - val_loss: 0.5058\n",
            "Epoch 14/100\n",
            "69/69 [==============================] - 10s 144ms/step - loss: 0.4829 - val_loss: 0.4776\n",
            "Epoch 15/100\n",
            "69/69 [==============================] - 11s 166ms/step - loss: 0.4526 - val_loss: 0.4764\n",
            "Epoch 16/100\n",
            "69/69 [==============================] - 11s 166ms/step - loss: 0.4263 - val_loss: 0.4269\n",
            "Epoch 17/100\n",
            "69/69 [==============================] - 11s 165ms/step - loss: 0.3968 - val_loss: 0.4028\n",
            "Epoch 18/100\n",
            "69/69 [==============================] - 10s 150ms/step - loss: 0.3733 - val_loss: 0.3713\n",
            "Epoch 19/100\n",
            "69/69 [==============================] - 11s 155ms/step - loss: 0.3525 - val_loss: 0.3822\n",
            "Epoch 20/100\n",
            "69/69 [==============================] - 11s 165ms/step - loss: 0.3297 - val_loss: 0.3561\n",
            "Epoch 21/100\n",
            "69/69 [==============================] - 11s 167ms/step - loss: 0.3074 - val_loss: 0.3447\n",
            "Epoch 22/100\n",
            "69/69 [==============================] - 11s 164ms/step - loss: 0.2899 - val_loss: 0.3136\n",
            "Epoch 23/100\n",
            "69/69 [==============================] - 10s 142ms/step - loss: 0.2686 - val_loss: 0.3476\n",
            "Epoch 24/100\n",
            "69/69 [==============================] - 11s 164ms/step - loss: 0.2538 - val_loss: 0.3133\n",
            "Epoch 25/100\n",
            "69/69 [==============================] - 12s 167ms/step - loss: 0.2382 - val_loss: 0.3145\n",
            "Epoch 26/100\n",
            "69/69 [==============================] - 12s 167ms/step - loss: 0.2231 - val_loss: 0.2579\n",
            "Epoch 27/100\n",
            "69/69 [==============================] - 11s 160ms/step - loss: 0.2110 - val_loss: 0.2757\n",
            "Epoch 28/100\n",
            "69/69 [==============================] - 10s 146ms/step - loss: 0.1953 - val_loss: 0.2667\n",
            "Epoch 29/100\n",
            "69/69 [==============================] - 11s 166ms/step - loss: 0.1898 - val_loss: 0.2385\n",
            "Epoch 30/100\n",
            "69/69 [==============================] - 11s 163ms/step - loss: 0.1743 - val_loss: 0.2357\n",
            "Epoch 31/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.1672 - val_loss: 0.2235\n",
            "Epoch 32/100\n",
            "69/69 [==============================] - 11s 153ms/step - loss: 0.1547 - val_loss: 0.2307\n",
            "Epoch 33/100\n",
            "69/69 [==============================] - 11s 154ms/step - loss: 0.1447 - val_loss: 0.2343\n",
            "Epoch 34/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.1360 - val_loss: 0.2255\n",
            "Epoch 35/100\n",
            "69/69 [==============================] - 12s 169ms/step - loss: 0.1313 - val_loss: 0.1924\n",
            "Epoch 36/100\n",
            "69/69 [==============================] - 11s 164ms/step - loss: 0.1229 - val_loss: 0.1896\n",
            "Epoch 37/100\n",
            "69/69 [==============================] - 10s 147ms/step - loss: 0.1151 - val_loss: 0.2110\n",
            "Epoch 38/100\n",
            "69/69 [==============================] - 11s 161ms/step - loss: 0.1097 - val_loss: 0.1806\n",
            "Epoch 39/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.1027 - val_loss: 0.2117\n",
            "Epoch 40/100\n",
            "69/69 [==============================] - 11s 167ms/step - loss: 0.0994 - val_loss: 0.1960\n",
            "Epoch 41/100\n",
            "69/69 [==============================] - 11s 163ms/step - loss: 0.0919 - val_loss: 0.1858\n",
            "Epoch 42/100\n",
            "69/69 [==============================] - 10s 144ms/step - loss: 0.0864 - val_loss: 0.1704\n",
            "Epoch 43/100\n",
            "69/69 [==============================] - 13s 197ms/step - loss: 0.0853 - val_loss: 0.2116\n",
            "Epoch 44/100\n",
            "69/69 [==============================] - 13s 190ms/step - loss: 0.0791 - val_loss: 0.1597\n",
            "Epoch 45/100\n",
            "69/69 [==============================] - 11s 167ms/step - loss: 0.0745 - val_loss: 0.1654\n",
            "Epoch 46/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0687 - val_loss: 0.1679\n",
            "Epoch 47/100\n",
            "69/69 [==============================] - 10s 151ms/step - loss: 0.0650 - val_loss: 0.1532\n",
            "Epoch 48/100\n",
            "69/69 [==============================] - 11s 158ms/step - loss: 0.0618 - val_loss: 0.1652\n",
            "Epoch 49/100\n",
            "69/69 [==============================] - 12s 169ms/step - loss: 0.0582 - val_loss: 0.1502\n",
            "Epoch 50/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0539 - val_loss: 0.1745\n",
            "Epoch 51/100\n",
            "69/69 [==============================] - 12s 170ms/step - loss: 0.0517 - val_loss: 0.1607\n",
            "Epoch 52/100\n",
            "69/69 [==============================] - 10s 144ms/step - loss: 0.0506 - val_loss: 0.1429\n",
            "Epoch 53/100\n",
            "69/69 [==============================] - 12s 167ms/step - loss: 0.0462 - val_loss: 0.1480\n",
            "Epoch 54/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0450 - val_loss: 0.1534\n",
            "Epoch 55/100\n",
            "69/69 [==============================] - 12s 169ms/step - loss: 0.0417 - val_loss: 0.1432\n",
            "Epoch 56/100\n",
            "69/69 [==============================] - 11s 157ms/step - loss: 0.0425 - val_loss: 0.1461\n",
            "Epoch 57/100\n",
            "69/69 [==============================] - 11s 154ms/step - loss: 0.0375 - val_loss: 0.1456\n",
            "Epoch 58/100\n",
            "69/69 [==============================] - 12s 169ms/step - loss: 0.0359 - val_loss: 0.1453\n",
            "Epoch 59/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0335 - val_loss: 0.1402\n",
            "Epoch 60/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0345 - val_loss: 0.1521\n",
            "Epoch 61/100\n",
            "69/69 [==============================] - 10s 145ms/step - loss: 0.0317 - val_loss: 0.1492\n",
            "Epoch 62/100\n",
            "69/69 [==============================] - 11s 165ms/step - loss: 0.0294 - val_loss: 0.1384\n",
            "Epoch 63/100\n",
            "69/69 [==============================] - 12s 169ms/step - loss: 0.0292 - val_loss: 0.1457\n",
            "Epoch 64/100\n",
            "69/69 [==============================] - 11s 167ms/step - loss: 0.0274 - val_loss: 0.1533\n",
            "Epoch 65/100\n",
            "69/69 [==============================] - 11s 161ms/step - loss: 0.0292 - val_loss: 0.1458\n",
            "Epoch 66/100\n",
            "69/69 [==============================] - 10s 148ms/step - loss: 0.0277 - val_loss: 0.1405\n",
            "Epoch 67/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0239 - val_loss: 0.1522\n",
            "Epoch 68/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0242 - val_loss: 0.1437\n",
            "Epoch 69/100\n",
            "69/69 [==============================] - 12s 167ms/step - loss: 0.0219 - val_loss: 0.1501\n",
            "Epoch 70/100\n",
            "69/69 [==============================] - 10s 152ms/step - loss: 0.0237 - val_loss: 0.1469\n",
            "Epoch 71/100\n",
            "69/69 [==============================] - 11s 157ms/step - loss: 0.0190 - val_loss: 0.1432\n",
            "Epoch 72/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0188 - val_loss: 0.1415\n",
            "Epoch 73/100\n",
            "69/69 [==============================] - 12s 171ms/step - loss: 0.0193 - val_loss: 0.1447\n",
            "Epoch 74/100\n",
            "69/69 [==============================] - 12s 169ms/step - loss: 0.0197 - val_loss: 0.1464\n",
            "Epoch 75/100\n",
            "69/69 [==============================] - 10s 145ms/step - loss: 0.0161 - val_loss: 0.1514\n",
            "Epoch 76/100\n",
            "69/69 [==============================] - 11s 165ms/step - loss: 0.0190 - val_loss: 0.1464\n",
            "Epoch 77/100\n",
            "69/69 [==============================] - 11s 165ms/step - loss: 0.0153 - val_loss: 0.1490\n",
            "Epoch 78/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0157 - val_loss: 0.1445\n",
            "Epoch 79/100\n",
            "69/69 [==============================] - 11s 160ms/step - loss: 0.0147 - val_loss: 0.1549\n",
            "Epoch 80/100\n",
            "69/69 [==============================] - 10s 148ms/step - loss: 0.0171 - val_loss: 0.1615\n",
            "Epoch 81/100\n",
            "69/69 [==============================] - 11s 166ms/step - loss: 0.0142 - val_loss: 0.1605\n",
            "Epoch 82/100\n",
            "69/69 [==============================] - 11s 165ms/step - loss: 0.0126 - val_loss: 0.1515\n",
            "Epoch 83/100\n",
            "69/69 [==============================] - 11s 167ms/step - loss: 0.0149 - val_loss: 0.1450\n",
            "Epoch 84/100\n",
            "69/69 [==============================] - 11s 156ms/step - loss: 0.0130 - val_loss: 0.1469\n",
            "Epoch 85/100\n",
            "69/69 [==============================] - 11s 153ms/step - loss: 0.0126 - val_loss: 0.1474\n",
            "Epoch 86/100\n",
            "69/69 [==============================] - 11s 165ms/step - loss: 0.0145 - val_loss: 0.1504\n",
            "Epoch 87/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0142 - val_loss: 0.1447\n",
            "Epoch 88/100\n",
            "69/69 [==============================] - 12s 167ms/step - loss: 0.0145 - val_loss: 0.1453\n",
            "Epoch 89/100\n",
            "69/69 [==============================] - 10s 148ms/step - loss: 0.0119 - val_loss: 0.1515\n",
            "Epoch 90/100\n",
            "69/69 [==============================] - 11s 162ms/step - loss: 0.0115 - val_loss: 0.1464\n",
            "Epoch 91/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0130 - val_loss: 0.1547\n",
            "Epoch 92/100\n",
            "69/69 [==============================] - 12s 168ms/step - loss: 0.0108 - val_loss: 0.1538\n",
            "Epoch 93/100\n",
            "69/69 [==============================] - 11s 161ms/step - loss: 0.0106 - val_loss: 0.1522\n",
            "Epoch 94/100\n",
            "69/69 [==============================] - 10s 146ms/step - loss: 0.0108 - val_loss: 0.1495\n",
            "Epoch 95/100\n",
            "69/69 [==============================] - 11s 165ms/step - loss: 0.0095 - val_loss: 0.1469\n",
            "Epoch 96/100\n",
            "69/69 [==============================] - 11s 165ms/step - loss: 0.0111 - val_loss: 0.1434\n",
            "Epoch 97/100\n",
            "69/69 [==============================] - 11s 166ms/step - loss: 0.0099 - val_loss: 0.1533\n",
            "Epoch 98/100\n",
            "69/69 [==============================] - 10s 152ms/step - loss: 0.0094 - val_loss: 0.1500\n",
            "Epoch 99/100\n",
            "69/69 [==============================] - 11s 154ms/step - loss: 0.0107 - val_loss: 0.1596\n",
            "Epoch 100/100\n",
            "69/69 [==============================] - 11s 165ms/step - loss: 0.0086 - val_loss: 0.1624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    }
  ]
}